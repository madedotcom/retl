% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bigquery.gs.R
\name{bqImportData}
\alias{bqImportData}
\title{Imports data from gs to BigQuery table}
\usage{
bqImportData(
  table,
  dataset = bqDefaultDataset(),
  path = "",
  append = TRUE,
  format = "CSV",
  compression = "GZIP",
  nskip = 1,
  bucket = Sys.getenv("GCS_BUCKET")
)
}
\arguments{
\item{table}{name of the table to extract}

\item{dataset}{name of the dataset}

\item{path}{path to the file in gs, defaults to {default-bucket}/{default-dataset}/table-name.csv.gz.}

\item{append}{defines whether data can be appended to the table with data}

\item{format}{The exported file format. Possible values
include "CSV", "NEWLINE_DELIMITED_JSON" and "AVRO". Tables with nested or
repeated fields cannot be exported as CSV.}

\item{compression}{The compression type to use for exported files. Possible
values include "GZIP", "DEFLATE", "SNAPPY", and "NONE". "DEFLATE" and
"SNAPPY" are only supported for Avro.}

\item{nskip}{number of rows to skip on importing the file}

\item{bucket}{name of the GCS bucket from where data will be loaded}
}
\value{
object of `bq_job`
}
\description{
Imports data from gs to BigQuery table
}
\seealso{
?bigrquery::bq_table_save
}
